{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e2f80-198a-46d9-87a4-c672f446494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "Lasso Regression:\n",
    "\n",
    "Definition: Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of \n",
    "linear regression that adds a penalty proportional to the absolute value of the magnitude of \n",
    "coefficients (L1 norm) to the cost function. This penalty term helps in shrinking some \n",
    "coefficients exactly to zero, effectively performing feature selection.\n",
    "Differences from Other Regression Techniques:\n",
    "\n",
    "OLS Regression: No penalty term, potentially leading to large coefficients and overfitting.\n",
    "Ridge Regression: Adds an L2 penalty term (squared magnitude of coefficients) but does not shrink coefficients to exactly zero. Primarily used to handle multicollinearity.\n",
    "Lasso Regression: Adds an L1 penalty term, which can shrink some coefficients to exactly zero, allowing for feature selection and sparsity in the model.\n",
    "\n",
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "Feature Selection:\n",
    "\n",
    "Advantage: The main advantage of Lasso Regression is its ability to perform automatic feature selection by shrinking some coefficients to zero. This results in a simpler, more interpretable model by excluding irrelevant or redundant features.\n",
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "Interpretation:\n",
    "\n",
    "Coefficients: Coefficients in Lasso Regression represent the change in the target variable for a one-unit change in the predictor variable, while other variables are held constant.\n",
    "Effect of Regularization: Coefficients that are exactly zero indicate that the corresponding features are excluded from the model. Non-zero coefficients indicate features that are considered important for predicting the target variable.\n",
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?\n",
    "Parameters:\n",
    "\n",
    "Regularization Parameter (\n",
    "𝜆\n",
    "λ): Controls the strength of the penalty term. A higher \n",
    "𝜆\n",
    "λ increases the amount of regularization, resulting in more coefficients being shrunk to zero, while a lower \n",
    "𝜆\n",
    "λ results in less regularization and potentially more non-zero coefficients.\n",
    "Effect on Model Performance: Proper tuning of \n",
    "𝜆\n",
    "λ is crucial as it affects the model’s balance between fitting the training data and avoiding overfitting. Cross-validation is often used to select the optimal \n",
    "𝜆\n",
    "λ.\n",
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "Non-Linear Regression:\n",
    "\n",
    "Application: Lasso Regression itself is a linear model and cannot directly handle non-linear relationships.\n",
    "Handling Non-Linearity:\n",
    "Feature Engineering: Transform non-linear relationships into linear features by adding polynomial terms or interaction terms.\n",
    "Kernel Methods: Combine Lasso with kernel methods to capture non-linearities in the data (e.g., using Lasso with polynomial features).\n",
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "Ridge Regression:\n",
    "\n",
    "Penalty: Adds L2 penalty \\ which shrinks coefficients but does not set any to zero.\n",
    "Usage: Primarily used to handle multicollinearity and prevent overfitting.\n",
    "Lasso Regression:\n",
    "\n",
    "Penalty: Adds L1 penalty which can shrink some coefficients exactly to zero, performing feature selection.\n",
    "Usage: Useful for both regularization and feature selection.\n",
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "Handling Multicollinearity:\n",
    "\n",
    "Effectiveness: Lasso Regression can handle multicollinearity by shrinking some coefficients to zero, which reduces the complexity of the model and minimizes the impact of collinear features.\n",
    "Mechanism: By setting some coefficients to zero, Lasso implicitly excludes some predictors from the model, which can help in reducing the effects of multicollinearity.\n",
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "Methods for Selecting \n",
    "𝜆\n",
    "λ:\n",
    "\n",
    "Cross-Validation: Use k-fold cross-validation to evaluate different values of \n",
    "𝜆\n",
    "λ and select the one that minimizes the cross-validation error.\n",
    "Grid Search: Perform a grid search over a range of \n",
    "𝜆\n",
    "λ values to find the optimal one.\n",
    "Regularization Path Algorithms: Use algorithms like LARS (Least Angle Regression) to compute the solution path efficiently for different values of \n",
    "𝜆\n",
    "λ.\n",
    "In summary, Lasso Regression is particularly useful for feature selection and handling multicollinearity, whereas Ridge Regression is more focused on regularization without feature exclusion. The choice between them depends on whether feature selection or coefficient shrinkage is more important for the specific problem at hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
