{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60778c50-ca71-48ec-bd2a-24df8f1d9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q1. What is Random Forest Regressor?\n",
    "Random Forest Regressor is an ensemble learning method used for regression tasks. It builds multiple decision trees during training and outputs the average prediction of the individual trees. \n",
    "This approach helps in improving the predictive accuracy and controlling overfitting compared to using a single decision tree.\n",
    "Q2. How does Random Forest Regressor reduce the risk of overfitting?\n",
    "Random Forest Regressor reduces the risk of overfitting through two main mechanisms:\n",
    "\n",
    "Bootstrap Sampling: Each tree is trained on a different bootstrap sample (random subset with replacement) of the training data, ensuring that the trees are diverse.\n",
    "Feature Randomness: At each split in the decision tree, a random subset of features is considered for splitting, rather than considering all features. This randomness helps in creating diverse trees, which collectively reduce the risk of overfitting.\n",
    "Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?\n",
    "In Random Forest Regressor, the predictions of multiple decision trees are aggregated by averaging:\n",
    "\n",
    "For Regression: The final prediction is the average of the predictions made by each decision tree in the forest.\n",
    "Q4. What are the hyperparameters of Random Forest Regressor?\n",
    "Key hyperparameters of Random Forest Regressor include:\n",
    "\n",
    "n_estimators: The number of trees in the forest.\n",
    "max_features: The number of features to consider when looking for the best split.\n",
    "max_depth: The maximum depth of the tree.\n",
    "min_samples_split: The minimum number of samples required to split an internal node.\n",
    "min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
    "bootstrap: Whether bootstrap samples are used when building trees.\n",
    "random_state: Seed used by the random number generator.\n",
    "criterion: The function to measure the quality of a split (e.g., \"mse\" for mean squared error).\n",
    "Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?\n",
    "Model Structure: A Decision Tree Regressor uses a single tree, whereas a Random Forest Regressor uses an ensemble of trees.\n",
    "Overfitting: Decision Tree Regressor is prone to overfitting, especially with deep trees. Random Forest Regressor, by averaging multiple trees, reduces overfitting.\n",
    "Prediction: Decision Tree Regressor makes predictions based on the structure of a single tree, while Random Forest Regressor aggregates the predictions of multiple trees, typically by averaging.\n",
    "Q6. What are the advantages and disadvantages of Random Forest Regressor?\n",
    "Advantages:\n",
    "\n",
    "Robustness to Overfitting: Due to averaging multiple trees, it generalizes better to new data.\n",
    "High Accuracy: Often achieves better performance than individual decision trees.\n",
    "Feature Importance: Can be used to estimate feature importance.\n",
    "Handles Missing Data: Can handle missing values and maintain accuracy for missing data.\n",
    "Disadvantages:\n",
    "\n",
    "Computational Complexity: Training and prediction can be computationally intensive.\n",
    "Interpretability: More difficult to interpret than a single decision tree.\n",
    "Resource Intensive: Requires more memory and storage.\n",
    "Q7. What is the output of Random Forest Regressor?\n",
    "The output of Random Forest Regressor is a continuous numerical value representing the predicted target variable for regression tasks. It is the average of the predictions made by all the trees in the ensemble.\n",
    "Q8. Can Random Forest Regressor be used for classification tasks?\n",
    "Random Forest Regressor is specifically designed for regression tasks. \n",
    "However, the Random Forest algorithm can be adapted for classification tasks using the Random Forest Classifier. In classification, the algorithm aggregates predictions by taking the majority vote among the trees rather than averaging the predictions. The basic principles remain the same, but the aggregation method differs to suit classification tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
