{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fadeefb-4159-4954-bacc-54fde4021c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "Gradient Boosting Regression is an ensemble learning technique used for regression tasks. It builds a model in a stage-wise manner by sequentially adding weak learners (typically decision trees) to minimize the loss function. The key idea is to improve the model by fitting the new learner to the residual errors of the combined ensemble of previous learners. \n",
    "This process is repeated until a specified number of iterations or until the error is sufficiently reduced.\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class SimpleGradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.initial_prediction = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        residuals = y - self.initial_prediction\n",
    "        for _ in range(self.n_estimators):\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            model.fit(X, residuals)\n",
    "            predictions = model.predict(X)\n",
    "            residuals -= self.learning_rate * predictions\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.full(X.shape[0], self.initial_prediction)\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Simple dataset\n",
    "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
    "y = np.array([3, 6, 4, 8, 7, 9, 13, 15, 10, 18])\n",
    "\n",
    "# Train the model\n",
    "model = SimpleGradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, predictions)\n",
    "r2 = r2_score(y, predictions)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def train_model(X, y, n_estimators, learning_rate, max_depth):\n",
    "    model = SimpleGradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    predictions = model.predict(X)\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    r2 = r2_score(y, predictions)\n",
    "    return mse, r2\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "best_score = float('inf')\n",
    "best_params = None\n",
    "for n_estimators in param_grid['n_estimators']:\n",
    "    for learning_rate in param_grid['learning_rate']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            model = train_model\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "A weak learner in Gradient Boosting is a simple model that performs slightly better than random guessing. Typically, decision trees with limited depth (often referred to as \"stumps\") are used as weak learners. The idea is that by combining many weak learners, each focusing on correcting the errors of the previous ones, a strong predictive model can be built.\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "The intuition behind the Gradient Boosting algorithm is to build a powerful model by sequentially adding weak learners, each trained to correct the errors made by the ensemble of previous learners. This is achieved by fitting each new learner to the residual errors (gradients) of the combined model. By iteratively minimizing the loss function, the algorithm refines the model, reducing bias and improving accuracy.\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "Gradient Boosting builds an ensemble of weak learners through the following steps:\n",
    "\n",
    "Initialize: Start with an initial prediction (often the mean of the target values for regression).\n",
    "Compute Residuals: Calculate the residuals (errors) between the predicted values and the true values.\n",
    "Fit Weak Learner: Train a weak learner (e.g., a decision tree) on the residuals.\n",
    "Update Model: Add the predictions of the weak learner to the ensemble, scaled by a learning rate.\n",
    "Repeat: Repeat steps 2-4 for a specified number of iterations or until the residuals are minimized.\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
