{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93780fc-e5d1-4433-b23a-95e622d8ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "The K-Nearest Neighbors (KNN) algorithm is a non-parametric, instance-based learning algorithm used for classification and regression tasks. It works by identifying the \n",
    "ùëò\n",
    "k nearest data points to a given query point and making predictions based on the majority class (for classification) or the average value (for regression) of these neighbors. The distance between points is typically measured using metrics like Euclidean distance.\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "Choosing the value of \n",
    "ùëò\n",
    "k in KNN is crucial for the algorithm's performance. Here are some common methods to choose \n",
    "ùëò\n",
    "k:\n",
    "\n",
    "Cross-Validation: Perform cross-validation on the training set to evaluate different \n",
    "ùëò\n",
    "k values and select the one that minimizes the validation error.\n",
    "Rule of Thumb: A common heuristic is to set \n",
    "ùëò\n",
    "k to the square root of the number of training samples (\n",
    "ùëõ\n",
    "n\n",
    "‚Äã\n",
    " ).\n",
    "Domain Knowledge: Use domain-specific knowledge to select an appropriate \n",
    "ùëò\n",
    "k.\n",
    "Iterative Testing: Start with a small \n",
    "ùëò\n",
    "k and gradually increase it, observing the effect on model performance.\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "KNN Classifier: Used for classification tasks. The predicted class for a query point is determined by the majority class among the \n",
    "ùëò\n",
    "k nearest neighbors.\n",
    "KNN Regressor: Used for regression tasks. The predicted value for a query point is the average of the values of the \n",
    "ùëò\n",
    "k nearest neighbors.\n",
    "Q4. How do you measure the performance of KNN?\n",
    "For Classification:\n",
    "\n",
    "Accuracy: The proportion of correctly classified instances.\n",
    "Precision, Recall, and F1-Score: Especially useful for imbalanced datasets.\n",
    "Confusion Matrix: Provides a detailed breakdown of prediction results.\n",
    "For Regression:\n",
    "    Mean Squared Error (MSE): Measures the average squared difference between predicted and actual values.\n",
    "Mean Absolute Error (MAE): Measures the average absolute difference between predicted and actual values.\n",
    "R-squared (\n",
    "ùëÖ\n",
    "2\n",
    "R \n",
    "2\n",
    " ): Indicates the proportion of variance explained by the model\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "The curse of dimensionality refers to the phenomenon where the distance between points becomes less meaningful as the number of dimensions (features) increases. In high-dimensional spaces, all points tend to be equidistant, making it difficult for the KNN algorithm to distinguish between close and distant neighbors. This can lead to poor performance and increased computational complexity.\n",
    "Q6. How do you handle missing values in KNN?\n",
    "Handling missing values in KNN can be approached in several ways:\n",
    "\n",
    "Imputation: Replace missing values with the mean, median, or mode of the feature.\n",
    "KNN Imputation: Use KNN to impute missing values by averaging (or using the most frequent value for classification) the values of the \n",
    "ùëò\n",
    "k nearest neighbors.\n",
    "Removal: If a small number of instances have missing values, they can be removed.\n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "KNN Classifier: Performs well for classification tasks, especially when the decision boundary is complex and non-linear. It is simple to implement and understand but can be sensitive to the choice of \n",
    "ùëò\n",
    "k and the distance metric.\n",
    "KNN Regressor: Suitable for regression tasks where the target variable is continuous. It tends to perform well when the relationship between the features and the target is smooth and continuous. However, it can be affected by outliers and may not capture complex relationships well.\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "Strengths:\n",
    "\n",
    "Simplicity: Easy to understand and implement.\n",
    "No Training Phase: The model is constructed during prediction, requiring no explicit training phase.\n",
    "Adaptability: Can be used for both classification and regression.\n",
    "Weaknesses:\n",
    "\n",
    "Computational Complexity: High memory and computation requirements during prediction, especially with large datasets.\n",
    "Sensitivity to Irrelevant Features: Performance can degrade if there are irrelevant or redundant features.\n",
    "Curse of Dimensionality: Performance decreases with increasing dimensionality.\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "Euclidean Distance: Measures the straight-line distance between two points in Euclidean space.\n",
    "Manhattan Distance: Measures the distance between two points along the axes at right angles (sum of the absolute differences of their coordinates).\n",
    "Q10. What is the role of feature scaling in KNN?\n",
    "Feature scaling is crucial in KNN because the algorithm relies on distance calculations to identify the nearest neighbors. If the features are on different scales, those with larger ranges will disproportionately influence the distance metric, leading to biased results. Common scaling techniques include:\n",
    "\n",
    "Standardization: Transform features to have zero mean and unit variance.\n",
    "Normalization: Scale features to a fixed range, typically [0, 1] or [-1, 1].\n",
    "By ensuring all features contribute equally to the distance calculations, feature scaling improves the performance and reliability of the KNN algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
